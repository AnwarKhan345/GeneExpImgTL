{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning using CNN\n",
    "\n",
    "In this notebook, a transfer learning (TL) approach is followed to solve a cancer prediction task on gene-expression samples from a concrete tumor type. We perform a TL approach by pre-training a CNN on the non-Lung cancer samples from the TCGA PanCancer dataset, and then fine-tune the model on the Lung cancer dataset (see `PanCancer_Lung_Split` notebook). As input data, we use the gene expression profiles modeled as an image (matrix) per sample, whose pixels (gene expression values) are neighbours depending on their KEGG-BRITE functional hierarchy (see `2-KEGG_BRITE_Treemap` R notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Auxiliary components\n",
    "from bio_dl_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progression free-interval\n",
    "\n",
    "Here, we predict the discrete progression-free interval (PFI) of each patient (sample), which correponds to a binary classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define survival variable of interest\n",
    "surv_variable = \"PFI\"\n",
    "surv_variable_time = \"PFI.time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-Lung cancer\n",
    "\n",
    "We only use the non-Lung tumor samples from the TCGA PanCancer dataset with the survival information of interest associated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9374, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load samples-info dataset\n",
    "Y_info = pd.read_hdf(\"../data/PanCancer/non_Lung_pancan.h5\", \n",
    "                     key=\"sample_type\")\n",
    "Y_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9374, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load survival clinical outcome dataset\n",
    "Y_surv = pd.read_hdf(\"../data/PanCancer/non_Lung_pancan.h5\", \n",
    "                     key=\"sample_clinical\")\n",
    "Y_surv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tumor     8771\n",
       "Normal     603\n",
       "Name: tumor_normal, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tumor-normal distribution\n",
    "Y_info.tumor_normal.value_counts(normalize=False, dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8771, 33)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter tumor samples from survival clinical outcome dataset\n",
    "Y_surv = Y_surv.loc[Y_info.tumor_normal==\"Tumor\"]\n",
    "Y_surv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8563, 33)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where surv_variable or surv_variable_time is NA\n",
    "Y_surv.dropna(subset=[surv_variable, surv_variable_time], inplace=True)\n",
    "Y_surv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     2992.000000\n",
       "mean       625.818516\n",
       "std        817.163242\n",
       "min          0.000000\n",
       "25%        188.000000\n",
       "50%        370.000000\n",
       "75%        729.250000\n",
       "max      10334.000000\n",
       "Name: PFI.time, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Event PFI samples time distribution\n",
    "Y_surv.loc[Y_surv.PFI==1.0]['PFI.time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     5571.000000\n",
       "mean      1050.329564\n",
       "std       1017.383207\n",
       "min          0.000000\n",
       "25%        388.000000\n",
       "50%        741.000000\n",
       "75%       1409.000000\n",
       "max      11217.000000\n",
       "Name: PFI.time, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Censored PFI samples time distribution\n",
    "Y_surv.loc[Y_surv.PFI==0.0]['PFI.time'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a discrete time class variable using the fixed-time point selected in `Lung_PFI_Prediction` notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7707,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = 230\n",
    "Y_surv_disc = Y_surv[['PFI', 'PFI.time']].apply(\n",
    "    lambda row: survival_fixed_time(time, row['PFI.time'], row['PFI']), axis=1)\n",
    "\n",
    "Y_surv_disc.dropna(inplace=True)\n",
    "Y_surv_disc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12222654729466718"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Event class fraction\n",
    "sum(Y_surv_disc)/len(Y_surv_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.9 s, sys: 1.98 s, total: 4.88 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load gene-exp images\n",
    "n_width = 175\n",
    "n_height = 175\n",
    "dir_name = \"gene_exp_treemap_\" + str(n_width) + \"_\" + str(n_height) + \"_npy/\"\n",
    "\n",
    "X_gene_exp = np.array([np.load(dir_name + s.replace(\"-\", \".\") + \".npy\") for s in Y_surv_disc.index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7707, 175, 175)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gene_exp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the binary class variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert discrete time survival numerical variables into binary variables\n",
    "Y_surv_disc_class = LabelEncoder().fit_transform(Y_surv_disc)\n",
    "np.unique(Y_surv_disc_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7707,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_surv_disc_class.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lung\n",
    "\n",
    "We also load the Lung tumor samples from the TCGA PanCancer dataset with the survival information of interest associated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 33)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load samples-info dataset\n",
    "Y_info_ft = pd.read_hdf(\"../data/PanCancer/Lung_pancan.h5\", key=\"sample\")\n",
    "# Load survival clinical outcome dataset\n",
    "Y_surv_ft = pd.read_hdf(\"../data/PanCancer/Lung_pancan.h5\", key=\"survival_outcome\")\n",
    "# Filter tumor samples from survival clinical outcome dataset\n",
    "Y_surv_ft = Y_surv_ft.loc[Y_info_ft.tumor_normal==\"Tumor\"]\n",
    "# Drop rows where surv_variable or surv_variable_time is NA\n",
    "Y_surv_ft.dropna(subset=[surv_variable, surv_variable_time], inplace=True)\n",
    "Y_surv_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = 230\n",
    "Y_surv_disc_ft = Y_surv_ft[['PFI', 'PFI.time']].apply(\n",
    "    lambda row: survival_fixed_time(time, row['PFI.time'], row['PFI']), axis=1)\n",
    "\n",
    "Y_surv_disc_ft.dropna(inplace=True)\n",
    "Y_surv_disc_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 347 ms, sys: 120 ms, total: 466 ms\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load gene-exp matrices\n",
    "X_gene_exp_ft = np.array([np.load(dir_name + s.replace(\"-\", \".\") + \".npy\") for s in Y_surv_disc_ft.index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855, 175, 175)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gene_exp_ft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the binary class variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert discrete time survival numerical variables into binary variables\n",
    "Y_surv_disc_class_ft = LabelEncoder().fit_transform(Y_surv_disc_ft)\n",
    "Y_surv_disc_class_ft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join PT and FT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use bayesian-optimization to perform the hyper-parameters tuning of a 2D-CNN architecture, using a cross-validation (CV) procedure. Random over-sampling is used both on pre-training and fine-tuning phases to deal with the severe class imbalance present in both non-Lung and Lung cancer datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Define training datasets\n",
    "X = X_gene_exp_ft\n",
    "y = Y_surv_disc_class_ft\n",
    "\n",
    "# Define the scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Define re-sampling method\n",
    "ros = RandomOverSampler(random_state=69)\n",
    "\n",
    "# Define image input shape\n",
    "image_shape = (*X.shape[1:3], 1)\n",
    "\n",
    "# Define custom transformer\n",
    "ft = FunctionTransformer(func=reshape_transformer, kw_args={'final_shape': image_shape})\n",
    "\n",
    "# Define cross-validation train-test splits\n",
    "cv_split = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=23)\n",
    "\n",
    "# Define evaluation metrics\n",
    "model_sel_metric = 'auc'\n",
    "eval_metric = {'auc': 'roc_auc', \n",
    "               'acc': make_scorer(opt_accuracy_score, needs_proba=True), \n",
    "               'sens': make_scorer(opt_sensitivity_score, needs_proba=True),\n",
    "               'spec': make_scorer(opt_specificity_score, needs_proba=True),\n",
    "               'prec': make_scorer(opt_precision_score, needs_proba=True),\n",
    "               'f1': make_scorer(opt_f1_score, needs_proba=True),\n",
    "               'mcc': make_scorer(opt_mcc_score, needs_proba=True),\n",
    "               'thres': make_scorer(opt_threshold_score, needs_proba=True)}\n",
    "\n",
    "# Bayesian-Optimization parameters\n",
    "n_iter = 100\n",
    "random_state = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "# Define the 2D-CNN hyperparameters space\n",
    "params_space = {\n",
    "    # PT hyper-param\n",
    "    # We assume that the base model contains 2 CNN layers and 1 dense layer\n",
    "    'clf__pt_params': hp.choice('pt_params', \n",
    "                [{'clf__dense_choice': hp.choice('dense_num_layers', \n",
    "                                         [{'clf__add_dense': 0,\n",
    "                                           'clf__dense_unit_1': hp.choice('1dense_unit_1', [120, 160, 200, 240]),\n",
    "\n",
    "                                           'clf__dense_dropout_1': hp.choice('1dense_dropout_1', [0.2, 0.4, 0.6, 0.8])\n",
    "                                          },\n",
    "                                          {'clf__add_dense': 1,\n",
    "                                           'clf__dense_unit_1': hp.choice('2dense_unit_1', [120, 160, 200, 240]),\n",
    "                                           'clf__dense_unit_2': hp.choice('2dense_unit_2', [25, 50, 75, 100]),\n",
    "\n",
    "                                           'clf__dense_activation_2': 'relu',\n",
    "\n",
    "                                           'clf__dense_dropout_1': hp.choice('2dense_dropout_1', [0.2, 0.4, 0.6, 0.8]),\n",
    "                                           'clf__dense_dropout_2': hp.choice('2dense_dropout_2', [0.2, 0.4, 0.6, 0.8])\n",
    "                                          }]),\n",
    "                'clf__cnn_filter_1': hp.choice('cnn_filter_1', [2, 4, 8, 12, 16]),\n",
    "                'clf__cnn_filter_2': hp.choice('cnn_filter_2', [8, 12, 16, 32, 40]),\n",
    "                              \n",
    "                # Input dim = 175, Output dim = [78, 86]\n",
    "                'clf__cnn_kernel_1': hp.choice('cnn_kernel_1', [4, 8, 12, 16, 20]),\n",
    "                # Input dim = [78, 86] (~ 175/2), Output dim = [32, 43]\n",
    "                'clf__cnn_kernel_2': hp.choice('cnn_kernel_2', [2, 4, 8, 12, 16]),\n",
    "                                   \n",
    "                'clf__cnn_dropout_1': hp.choice('cnn_dropout_1', [0.2, 0.4, 0.6, 0.8]),\n",
    "                'clf__cnn_dropout_2': hp.choice('cnn_dropout_2', [0.2, 0.4, 0.6, 0.8]),\n",
    "                  \n",
    "               'clf__batch_size': hp.choice('batch_size_pt', [64, 128, 256, 384, 512]),\n",
    "               'clf__lr': hp.loguniform('lr_pt', np.log(1e-3), np.log(1e-1)),\n",
    "               # Re-sampling hyper-params\n",
    "               're_sample_pt__sampling_strategy': hp.choice('sampling_strategy_pt', [1, 1/2, 1/3, 1/4])}]),\n",
    "    'clf__ft_params': hp.choice('ft_params', \n",
    "                [{'clf__batch_size': hp.choice('batch_size_ft', [32, 80, 128, 192, 256]),\n",
    "                  'clf__lr': hp.loguniform('lr_ft', np.log(5e-4), np.log(1e-1)),\n",
    "                  # Re-sampling hyper-params\n",
    "                  're_sample_ft__sampling_strategy': hp.choice('sampling_strategy_ft', [1, 1/2, 1/3, 1/4])}])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define PT 2D-CNN estimator\n",
    "pre_model_path = 'keras-models/ros_new_cnn_pt_ft_disc_pfi_x.h5'\n",
    "cnn_pt = SklearnCNN(input_shape=image_shape, cnn_filter={}, cnn_kernel={}, cnn_pool={1: 2, 2: 2}, \n",
    "                 cnn_activation={1: 'relu', 2: 'relu'}, cnn_dropout={}, dense_unit={}, dense_activation={1: 'relu'}, \n",
    "                 dense_dropout={}, output_unit=1, output_activation='sigmoid', \n",
    "                 optimizer_name='adam', loss_function='binary_crossentropy', epoch=200, patience=10, verbose=0, \n",
    "                 model_path=pre_model_path)\n",
    "\n",
    "cnn_pipe_pt = Pipeline([('scaler', sc), ('re_sample_pt', ros), ('transf', ft), ('clf', cnn_pt)])\n",
    "\n",
    "# Define FT 2D-CNN estimator\n",
    "cnn_ft = SklearnFT(pre_layer=17, n_freeze=0, dense_unit={}, dense_activation={}, dense_dropout={},\n",
    "                      output_unit=1, output_activation='sigmoid', optimizer_name='adam', \n",
    "                      loss_function='binary_crossentropy', epoch=200, patience=10, verbose=0,\n",
    "                      pre_model=pre_model_path, \n",
    "                      model_path='keras-models/ros_new_cnn_pt_ft_disc_pfi_x_fine.h5')\n",
    "\n",
    "cnn_pipe_ft = Pipeline([('scaler', sc), ('re_sample_ft', ros), ('transf', ft), ('clf', cnn_ft)])\n",
    "\n",
    "# Define Bayesian-Optimization\n",
    "hyper_search = HyperoptCV_TL(estimator_pt=cnn_pipe_pt, \n",
    "                          X_pt=X_gene_exp.reshape(X_gene_exp.shape[0], X_gene_exp.shape[1]*X_gene_exp.shape[2]), \n",
    "                          y_pt=Y_surv_disc_class, estimator_ft=cnn_pipe_ft, \n",
    "                          hyper_space=params_space, cv=cv_split, scoring=eval_metric, opt_metric=model_sel_metric,\n",
    "                          n_iter=n_iter, random_seed=random_state, verbose_file = 'class_imb_pt_new_pt_ft_verbose.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_trial = hyper_search.model_selection(X.reshape(X.shape[0], X.shape[1]*X.shape[2]), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__ft_params': {'clf__batch_size': 128,\n",
       "  'clf__lr': 0.0014544476342579621,\n",
       "  're_sample_ft__sampling_strategy': 0.5},\n",
       " 'clf__pt_params': {'clf__batch_size': 256,\n",
       "  'clf__cnn_dropout_1': 0.8,\n",
       "  'clf__cnn_dropout_2': 0.2,\n",
       "  'clf__cnn_filter_1': 12,\n",
       "  'clf__cnn_filter_2': 12,\n",
       "  'clf__cnn_kernel_1': 8,\n",
       "  'clf__cnn_kernel_2': 4,\n",
       "  'clf__dense_choice': {'clf__add_dense': 1,\n",
       "   'clf__dense_activation_2': 'relu',\n",
       "   'clf__dense_dropout_1': 0.8,\n",
       "   'clf__dense_dropout_2': 0.6,\n",
       "   'clf__dense_unit_1': 160,\n",
       "   'clf__dense_unit_2': 100},\n",
       "  'clf__lr': 0.0029640098355867947,\n",
       "  're_sample_pt__sampling_strategy': 0.5}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial['result']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F-1</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Prec</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Sens</th>\n",
       "      <th>Spec</th>\n",
       "      <th>Thres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.688538</td>\n",
       "      <td>0.732602</td>\n",
       "      <td>0.310144</td>\n",
       "      <td>0.246918</td>\n",
       "      <td>0.217582</td>\n",
       "      <td>0.675147</td>\n",
       "      <td>0.675147</td>\n",
       "      <td>0.689874</td>\n",
       "      <td>0.255182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.131418</td>\n",
       "      <td>0.052570</td>\n",
       "      <td>0.067082</td>\n",
       "      <td>0.075432</td>\n",
       "      <td>0.084078</td>\n",
       "      <td>0.145862</td>\n",
       "      <td>0.145862</td>\n",
       "      <td>0.157680</td>\n",
       "      <td>0.231786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.415205</td>\n",
       "      <td>0.631048</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.136791</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.367742</td>\n",
       "      <td>0.014001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.580409</td>\n",
       "      <td>0.702353</td>\n",
       "      <td>0.257525</td>\n",
       "      <td>0.190115</td>\n",
       "      <td>0.151974</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.558065</td>\n",
       "      <td>0.104518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.730242</td>\n",
       "      <td>0.303977</td>\n",
       "      <td>0.246270</td>\n",
       "      <td>0.204870</td>\n",
       "      <td>0.696691</td>\n",
       "      <td>0.696691</td>\n",
       "      <td>0.676372</td>\n",
       "      <td>0.140271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.808480</td>\n",
       "      <td>0.771069</td>\n",
       "      <td>0.352544</td>\n",
       "      <td>0.307320</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.829032</td>\n",
       "      <td>0.360953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.842742</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.497240</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.847045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ACC        AUC        F-1        MCC       Prec     Recall  \\\n",
       "count  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000   \n",
       "mean    0.688538   0.732602   0.310144   0.246918   0.217582   0.675147   \n",
       "std     0.131418   0.052570   0.067082   0.075432   0.084078   0.145862   \n",
       "min     0.415205   0.631048   0.218750   0.136791   0.125000   0.312500   \n",
       "25%     0.580409   0.702353   0.257525   0.190115   0.151974   0.625000   \n",
       "50%     0.684211   0.730242   0.303977   0.246270   0.204870   0.696691   \n",
       "75%     0.808480   0.771069   0.352544   0.307320   0.250000   0.764706   \n",
       "max     0.912281   0.842742   0.545455   0.497240   0.562500   0.882353   \n",
       "\n",
       "            Sens       Spec      Thres  \n",
       "count  50.000000  50.000000  50.000000  \n",
       "mean    0.675147   0.689874   0.255182  \n",
       "std     0.145862   0.157680   0.231786  \n",
       "min     0.312500   0.367742   0.014001  \n",
       "25%     0.625000   0.558065   0.104518  \n",
       "50%     0.696691   0.676372   0.140271  \n",
       "75%     0.764706   0.829032   0.360953  \n",
       "max     0.882353   0.954545   0.847045  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = best_trial['result']['test_score']\n",
    "res = pd.DataFrame({'AUC': scores['test_auc'], \n",
    "              'ACC': scores['test_acc'], \n",
    "              'Sens': scores['test_sens'], \n",
    "              'Recall': scores['test_recall'], \n",
    "              'Spec': scores['test_spec'],\n",
    "              'Prec': scores['test_prec'],\n",
    "              'F-1': scores['test_f1'],\n",
    "              'MCC': scores['test_mcc'],\n",
    "              'Thres': scores['test_thres']})\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "file_path = 'results/ros_pt_ft_new_disc_pfi_lung_ft_100_iter_rep_kfold.csv'\n",
    "res.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
